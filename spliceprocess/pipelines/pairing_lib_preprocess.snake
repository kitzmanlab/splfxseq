######################################################
# Configuration values you must specify:
#
#
# > lib_table  
#   one row per sequencing library to be processed
#   required columns
#   - libname   (this nust be uniq; if you are running the same lib across multiple batches you need to 
#   - fastq_fwd  (path to forward read fastq file)
#   - fastq_rev  (path to reverse read fastq file)
#   - stuffer  = one of: bsmbi, paqci, spry1
#
# > outdir
# 
#######################################################
#  optional config values:
#
#   prefix 
#
#   cutadapt_stage1_opts  - options passed to cutadapt 
#   cutadapt_stage2_opts  - options passed to cutadapt 
#   
#   starcode_options
#
#   stop_after_nreads - for debugging purposes, stop after this number of reads per library

#######################################################
# For example, to run:
#  

import os.path as op
import os
import pandas as pd
import pysam 
import Bio.Seq

########

localrules: outtbl

assert 'lib_table' in config, 'must specify sample table'


STUFFER_STAGE1 = {
    'paqci':'AGTGCCATCTTGGATCTCCA...CCACAGATGCAGGTGCACCG',
    'bsmbi':'AGTGCCATCTTGGATCTCCA...CCACAGAGACGCACCGATGC',
    'spry1':'AGTGCCATCTTGGATCTCCA...CCACACCAGCCACCACCTTC', 
}


LAD_STAGE2 = ['cgatgcaggtgactgagcatcggtgcacctgc', 'gatccaagatggcactccagatc']

PREFIX = config['prefix'] if 'prefix' in config  else ''
OUT_DIR = config['outdir']

CUTADAPT_STAGE1_OPTS = config['cutadapt_stage1_opts'] if 'cutadapt_stage1_opts' in config else "-e 0.1 -O 30 --minimum-length 10 -q 10 "

CUTADAPT_STAGE2_OPTS = config['cutadapt_stage2_opts'] if 'cutadapt_stage2_opts' in config else "-e 0.1 -O 20 --minimum-length 25 -q 10 "

STOP_AFTER_NREADS = int(config['stop_after_nreads']) if 'stop_after_nreads' in config else -1 

if 'starcode_options' not in config:
    config['starcode_options'] = ' -d 1 '


########
# load and check sample table.

l_reqd_cols = [ 'libname', 'fastq_fwd', 'fastq_rev', 'stuffer' ]
tbl_lib = pd.read_table( config['lib_table'] )
assert all( [ col in tbl_lib.columns for col in l_reqd_cols ] ), 'lib table must have columns: '+','.join(l_reqd_cols)
assert len(set(tbl_lib['libname'])) == tbl_lib.shape[0], 'all libname entries must be unique'
    
tbl_lib = tbl_lib.set_index( 'libname',drop=False )

lLibs = tbl_lib['libname'].unique()

########
# expected output files

assert 'outdir' in config, 'must specify output directory'

# final output fastqs
l_out_bcread =  expand('{}/trim2/{}{{libname}}.bc.fq.gz'.format(OUT_DIR,PREFIX), libname=lLibs)
l_out_taggedinsread = expand('{}/trim2/{}{{libname}}.ins_tag.fq.gz'.format(OUT_DIR,PREFIX), libname=lLibs)
l_out_histo = expand('{}/histo/{}{{libname}}.bcclusters.txt.gz'.format(OUT_DIR,PREFIX), libname=lLibs)
    
outRpt = OUT_DIR+'/'+PREFIX+'clip_report.txt'

lOutFiles = [outRpt] + l_out_bcread + l_out_taggedinsread  + l_out_histo

########

rule all:
    input:
        lOutFiles


rule countinputs:
    # just count the inputs
    input:
        fq_fwd = lambda wc: tbl_lib.loc[ wc.libname ][ 'fastq_fwd' ],
    output:
        counts_input = op.join(OUT_DIR,'fastq/'+PREFIX+'{libname}.input.linects.txt')
    #log: op.join(OUT_DIR,'fastq/'+PREFIX+'{libname}.countinputs.log')
    threads: 8
    resources:
        mem_per_cpu="4gb", 
        cpus="8", 
        time="0:30:00"
    run:
        if 'stop_after_nreads' in config:
            nlines = 4 * int(config['stop_after_nreads'])
            shell("""
                set +o pipefail;  #fun fact, when head breaks the pipe is returns nonzero exit code
                pigz -d -c -p8 {input.fq_fwd} | head -n {nlines} | wc -l > {output.counts_input}
            """)
        else:
            shell("""
                pigz -d -c -p8 {input.fq_fwd} | wc -l > {output.counts_input}
            """)

rule clip_stage1:
    input:
        fq_fwd = lambda wc: tbl_lib.loc[ wc.libname ][ 'fastq_fwd' ],
        fq_rev = lambda wc: tbl_lib.loc[ wc.libname ][ 'fastq_rev' ]
    output:
        temp_head_fq_fwd = temp( op.join( OUT_DIR, 'trim1/'+PREFIX+'{libname}.ins.head.fq' )),
        temp_head_fq_rev = temp( op.join( OUT_DIR, 'trim1/'+PREFIX+'{libname}.bc.head.fq' )),
        fq_ins = temp( op.join( OUT_DIR, 'trim1/'+PREFIX+'{libname}.ins.fq.gz' )),
        fq_bc = temp( op.join( OUT_DIR, 'trim1/'+PREFIX+'{libname}.bc.fq.gz' )),
        cutad_log = op.join( OUT_DIR, 'trim1/'+PREFIX+'{libname}.trim1.json' )
    params:
        stuffseq = lambda wc: STUFFER_STAGE1[tbl_lib.loc[ wc.libname, 'stuffer'] ]
    threads: 16
    resources:
        mem_per_cpu="2gb", 
        cpus="16", 
        time="3:00:00"
    # log: op.join(OUT_DIR,'trim1/'+PREFIX+'{libname}.trim1.err')
    run:
        clipcmd_first_part = 'cutadapt -g {params.stuffseq} --discard-untrimmed --pair-filter=first -j {threads} %s -o {output.fq_bc} -p {output.fq_ins} --json {output.cutad_log} '%( CUTADAPT_STAGE1_OPTS )
    
        if STOP_AFTER_NREADS > 0:
            clipcmd = clipcmd_first_part + '{output.temp_head_fq_rev} {output.temp_head_fq_fwd} '
        else:
            clipcmd = clipcmd_first_part + '{input.fq_rev} {input.fq_fwd} '

        if STOP_AFTER_NREADS > 0:
            nlines = 4 * STOP_AFTER_NREADS
            shell("""   
                set +o pipefail;  #fun fact, when head breaks the pipe is returns nonzero exit code
                pigz -d -c -p8 {input.fq_fwd} | head -n {nlines} > {output.temp_head_fq_fwd}
                pigz -d -c -p8 {input.fq_rev} | head -n {nlines} > {output.temp_head_fq_rev}
                %s 
            """%(
                clipcmd
            ))
        else:
            shell("""
                touch {output.temp_head_fq_fwd} {output.temp_head_fq_rev} ;
                %s
            """%(clipcmd))

rule clip_stage2:
    input:
        fq_bc = rules.clip_stage1.output.fq_bc, 
        fq_ins = rules.clip_stage1.output.fq_ins,
    output:
        fq_ins = temp( op.join( OUT_DIR, 'trim2/'+PREFIX+'{libname}.ins.fq.gz' )),
        fq_bc = op.join( OUT_DIR, 'trim2/'+PREFIX+'{libname}.bc.fq.gz' ),
        cutad_log = op.join( OUT_DIR, 'trim2/'+PREFIX+'{libname}.trim2.json' )
    threads: 16
    resources:
        mem_per_cpu="2gb", 
        cpus="16", 
        time="3:00:00"
    # log: op.join(OUT_DIR,'trim2/'+PREFIX+'{libname}.trim2.err')
    run:
        clip_opts = " ".join( [ "-a '%s'"%seq for seq in LAD_STAGE2 ] )

        clipcmd = 'cutadapt %s --pair-filter=first -j {threads} %s -o {output.fq_ins} -p {output.fq_bc} --json {output.cutad_log} {input.fq_ins} {input.fq_bc}  '%( clip_opts, CUTADAPT_STAGE2_OPTS )
    
        shell(clipcmd) 

rule tag_ins:
    input:
        fq_bc = rules.clip_stage2.output.fq_bc,
        fq_ins = rules.clip_stage2.output.fq_ins,
    output:
        fq_ins = op.join( OUT_DIR, 'trim2/'+PREFIX+'{libname}.ins_tag.fq.gz' ),
    threads: 4
    resources:
        mem_per_cpu="2gb", 
        cpus="4", 
        time="5:00:00"
    shell:
        """
        tag_insert_read_name \
            --in_tag_read {input.fq_bc} \
            --in_insert_read {input.fq_ins} \
            --out_insert_read {output.fq_ins}
        """

rule starcode:
    input:
        bc_read = rules.clip_stage2.output.fq_bc
    output:
        bc_clust_histo_gz = op.join( OUT_DIR, 'histo/'+PREFIX+'{libname}.bcclusters.txt.gz' ),
        wfall_plot = op.join( OUT_DIR, 'bcseq/plots/waterfall_persamp/'+PREFIX+'{libname}.png' )
    params:
        starcode_options = config['starcode_options'],
        bc_clust_histo = op.join( OUT_DIR, 'histo/'+PREFIX+'{libname}.bcclusters.txt' )
    threads: 24
    resources:
        mem_per_cpu="5gb", 
        cpus="24", 
        time="1:00:00"
    run:
        if input.bc_read.endswith('gz'):
            unc_bc_read = '<( echo -e "@emptyread\\nNNNNNNNNNN\\n+\\nIIIIIIIIII"; pigz -d -c -p {resources.cpus} {input.bc_read} )'.format(**locals())
        else:
            unc_bc_read = '<( echo -e "@emptyread\\nNNNNNNNNNN\\n+\\nIIIIIIIIII"; cat {input.bc_read} ) '

        shell("""
            starcode \
                -t {resources.cpus} \
                --print-clusters \
                {params.starcode_options} \
                -i {unc_bc_read} \
                -o {params.bc_clust_histo}

            cat {params.bc_clust_histo} | pigz -c -p {resources.cpus} > {output.bc_clust_histo_gz}

            rm {params.bc_clust_histo}

            plot_tag_count_histos \
                --linCountHisto {output.bc_clust_histo_gz} \
                --lNames {wildcards.libname} \
                --annotMaxY \
                --outPlot {output.wfall_plot}
        """)


# gather up read counts and put in a summary table 
rule outtbl:
    input:
        bc_read = expand(rules.clip_stage2.output.fq_bc, libname=lLibs),
        ins_read = expand(rules.tag_ins.output.fq_ins, libname=lLibs),
        bcclust_histo=expand(rules.starcode.output.bc_clust_histo_gz, libname=lLibs),
    output:
        table_out=outRpt
    run:
        # shell( 'touch {output.table_out} ')

        tbl_out = tbl_lib.copy()
        tbl_out['bc_read'] = list(input.bc_read)
        tbl_out['insert_read'] = list(input.ins_read)
        tbl_out['bc_histo'] = list(input.bcclust_histo)

        tbl_out.to_csv(output.table_out,sep='\t',index=False)


        # tbl_out['nreads_input'] = [int(open(fn,'r').readline().strip().split()[0]) for fn in input.counts_input ]
        # assert (all(tbl_out['nreads_input']%4)==0), '#lines in input one of the input fqs not div by 4!'
        # tbl_out['nreads_input']=(tbl_out['nreads_input']/4).astype(int)
        

        # tbl_out['nreads_postoverlap'] = [int(open(fn,'r').readline().strip().split()[0]) for fn in input.counts_postovl ]
        # assert (all(tbl_out['nreads_postoverlap']%4)==0), '#lines in input one of the input fqs not div by 4!'
        # tbl_out['nreads_postoverlap']=(tbl_out['nreads_postoverlap']/4).astype(int)
        
        # tbl_out['nreads_poststufferrmv'] = [ findline_cutadapt_log(fn) for fn in input.counts_postdestuff ]
        # tbl_out['nreads_postprimerrmv'] = [ findline_cutadapt_log(fn) for fn in input.counts_postpritrim ]

        # tbl_out = tbl_out.set_index(KEYCOL,drop=False)

        # # gather alignment stats
        # align_stats = [ pd.read_csv(fn,index_col=0) for fn in input.counts_align ]
        # align_stats = pd.concat( align_stats, axis=1 ).transpose()
        # tbl_out = pd.concat( [tbl_out, align_stats], axis=1 )

        # #           'aligns_pass':n_pass,
        # #   'aligns_fail_multiparts':n_filt_multiparts,
        # #   'aligns_fail_longdel':n_filt_longdel,
        # #   'aligns_fail_longins':n_filt_longins,
        # #   'aligns_fail_longclip':n_filt_longclip,
        # #   'aligns_fail_unaligned':n_filt_unaligned


        # tbl_out['frac_fail_overlap'] = 1 - tbl_out['nreads_postoverlap'] / tbl_out['nreads_input']
        # tbl_out['frac_fail_stuffer'] = 1 - tbl_out['nreads_poststufferrmv'] / tbl_out['nreads_postoverlap']
        # tbl_out['frac_fail_primerrmv'] = 1 - tbl_out['nreads_postprimerrmv'] / tbl_out['nreads_poststufferrmv']

        # tbl_out.to_csv(output['table_out'],sep='\t',index=False)
        
        